{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MACHINE LEARNING - PROJECT 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from proj1_helpers import *\n",
    "from implementations import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOAD THE TRAINING DATA INTO FEATURE MATRIX, CLASS LABELS and EVENT IDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.94 s, sys: 474 ms, total: 8.41 s\n",
      "Wall time: 8.45 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Load train data and supply path\n",
    "DATA_TRAIN_PATH = '../data/train.csv'\n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FEATURES ENGINEERING & DATA PROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#constants definitions\n",
    "PRI_JET_NUM_IDX = 22   \n",
    "PRI_JET_NUM_VALUES = range(4)\n",
    "NUMBER_GROUPS = len(PRI_JET_NUM_VALUES)\n",
    "NBR_FEATURES = 30\n",
    "UNDEFINED_VALUE = -999.\n",
    "print_results = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#seperating the data within the four groups\n",
    "jet_groups_indices = [tX[:, PRI_JET_NUM_IDX] == pri_jet_num_value for pri_jet_num_value in PRI_JET_NUM_VALUES]\n",
    "TX_arr = [tX[group_indices] for group_indices in jet_groups_indices]\n",
    "Y_arr, TX_arr = zip(*[(y[group_indices], tX[group_indices]) for group_indices in jet_groups_indices])\n",
    "Y_arr, TX_arr = list(Y_arr), list(TX_arr)\n",
    "\n",
    "#collecting the indices of the undefined features for each group\n",
    "undefined_features = [[], [], [], []]\n",
    "for group_idx in range(NUMBER_GROUPS):\n",
    "    tx = TX_arr[group_idx]\n",
    "    for feature_idx in range(NBR_FEATURES):\n",
    "        feature_column = tx[:, feature_idx]\n",
    "        if np.all(feature_column == UNDEFINED_VALUE):\n",
    "            undefined_features[group_idx].append(feature_idx)\n",
    "\n",
    "#computing the std of the features for each group\n",
    "STDS = [np.std(TX_arr[i], axis = 0) for i in range(NUMBER_GROUPS)]\n",
    "\n",
    "#collecting the indices of the features with no variance (i.e. constant features) within each groups\n",
    "cst_features = [[], [], [], []]\n",
    "for group_idx, elem in enumerate(STDS):\n",
    "    for feature_idx, std in enumerate(elem):\n",
    "        if std == 0. and feature_idx not in undefined_features[group_idx]:\n",
    "            cst_features[group_idx].append(feature_idx)\n",
    "\n",
    "#deleting the features either undefined or with no variance (i.e. constant features) within each groups\n",
    "features_to_keep = ([[x for x in range(NBR_FEATURES) \n",
    "                      if x not in undefined_features[group_idx] and x not in cst_features[group_idx]] \n",
    "                      for group_idx in range(NUMBER_GROUPS)])\n",
    "TX_arr = [TX_arr[group_idx][:, features_to_keep[group_idx]] for group_idx in range(NUMBER_GROUPS)]\n",
    "\n",
    "#standardizing the data\n",
    "for tx in TX_arr:\n",
    "    tx -= np.mean(tx, axis = 0)\n",
    "    tx /= np.std(tx, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features for group 0 :  18\n",
      "Number of features for group 1 :  22\n",
      "Number of features for group 2 :  29\n",
      "Number of features for group 3 :  29\n"
     ]
    }
   ],
   "source": [
    "# Print the remaining number of features for each JET NUMBER\n",
    "for i in PRI_JET_NUM_VALUES:\n",
    "    print(f\"Number of features for group {i} : \", len(features_to_keep[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 7, 8, 9, 10, 11, 13, 14, 15, 16, 17, 18, 19, 20, 21]\n",
      "[0, 1, 2, 3, 7, 8, 9, 10, 11, 13, 14, 15, 16, 17, 18, 19, 20, 21, 23, 24, 25, 29]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 23, 24, 25, 26, 27, 28, 29]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 23, 24, 25, 26, 27, 28, 29]\n"
     ]
    }
   ],
   "source": [
    "print(features_to_keep[0])\n",
    "print(features_to_keep[1])\n",
    "print(features_to_keep[2])\n",
    "print(features_to_keep[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False False False ... False  True  True]\n",
      "[ 0.  0.  0. ...  0. -1. -1.]\n"
     ]
    }
   ],
   "source": [
    "print(jet_groups_indices[0])\n",
    "y_p = np.empty(250_000)\n",
    "y_p[jet_groups_indices[0]] = -1\n",
    "print(y_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Data with different models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LEAST SQUARES WITH GRADIENT DESCENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gamma = 0.01\n",
      "    For JET_NB 0, the obtained loss is 0.39229353135155837\n",
      "    For JET_NB 1, the obtained loss is 0.4132027466118129\n",
      "    For JET_NB 2, the obtained loss is 0.3551679496212518\n",
      "    For JET_NB 3, the obtained loss is 0.4356752189969854\n",
      "Gamma = 0.001\n",
      "    For JET_NB 0, the obtained loss is 0.4071890187854611\n",
      "    For JET_NB 1, the obtained loss is 0.42394948381609865\n",
      "    For JET_NB 2, the obtained loss is 0.3670042247722906\n",
      "    For JET_NB 3, the obtained loss is 0.44420031191889164\n",
      "Gamma = 0.0001\n",
      "    For JET_NB 0, the obtained loss is 0.4443662318323023\n",
      "    For JET_NB 1, the obtained loss is 0.4574097786010708\n",
      "    For JET_NB 2, the obtained loss is 0.41216640807377397\n",
      "    For JET_NB 3, the obtained loss is 0.4713233200193401\n",
      "Gamma = 1e-05\n",
      "    For JET_NB 0, the obtained loss is 0.49126372563414994\n",
      "    For JET_NB 1, the obtained loss is 0.4930816662458995\n",
      "    For JET_NB 2, the obtained loss is 0.48454918310149103\n",
      "    For JET_NB 3, the obtained loss is 0.49572853378650933\n",
      "CPU times: user 6min 29s, sys: 29.4 s, total: 6min 58s\n",
      "Wall time: 1min 47s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "gammas = [0.01, 0.001, 0.0001, 0.00001]\n",
    "max_iters = 2000\n",
    "\n",
    "# Iterate over some gammas to find the best possible values\n",
    "for gamma in gammas:\n",
    "    print(f\"Gamma = {gamma}\")\n",
    "    \n",
    "    # Iterate over all jet numbers dataframes\n",
    "    for i in range(len(features_to_keep)):\n",
    "        initial_w = np.zeros(len(features_to_keep[i]))\n",
    "        y = Y_arr[i]\n",
    "        tx = TX_arr[i]\n",
    "        weights, loss = least_squares_GD(y, tx, initial_w, max_iters, gamma)\n",
    "        print(f\"    For JET_NB {i}, the obtained loss is {loss:>15}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LEAST SQUARES  WITH STOCHASTIC GRADIENT DESCENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gamma = 0.1\n",
      "    For JET_NB 0, the obtained loss is 1406874.6402069617\n",
      "    For JET_NB 1, the obtained loss is 8.905399313440892e+44\n",
      "    For JET_NB 2, the obtained loss is 2.62722134033137e+92\n",
      "    For JET_NB 3, the obtained loss is 3.6558229573636576e+99\n",
      "Gamma = 0.01\n",
      "    For JET_NB 0, the obtained loss is 0.46456667812924324\n",
      "    For JET_NB 1, the obtained loss is 0.48520943306089004\n",
      "    For JET_NB 2, the obtained loss is 0.40874983381282415\n",
      "    For JET_NB 3, the obtained loss is 0.5521115844435248\n",
      "Gamma = 0.001\n",
      "    For JET_NB 0, the obtained loss is 0.41290183723982854\n",
      "    For JET_NB 1, the obtained loss is 0.4297899798480046\n",
      "    For JET_NB 2, the obtained loss is 0.3730644250016471\n",
      "    For JET_NB 3, the obtained loss is 0.45520388622179947\n",
      "Gamma = 0.0001\n",
      "    For JET_NB 0, the obtained loss is 0.45264898655691643\n",
      "    For JET_NB 1, the obtained loss is 0.4645855495261453\n",
      "    For JET_NB 2, the obtained loss is 0.42308955262164477\n",
      "    For JET_NB 3, the obtained loss is 0.4773245748345276\n",
      "Gamma = 1e-05\n",
      "    For JET_NB 0, the obtained loss is 0.49344888278524623\n",
      "    For JET_NB 1, the obtained loss is 0.4949203144182571\n",
      "    For JET_NB 2, the obtained loss is 0.4880377631966352\n",
      "    For JET_NB 3, the obtained loss is 0.4969312480592395\n",
      "CPU times: user 6min 1s, sys: 21.6 s, total: 6min 23s\n",
      "Wall time: 6min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "gammas = [0.1, 0.01, 0.001, 0.0001, 0.00001]\n",
    "max_iters = 1500\n",
    "batch_size = 1\n",
    "\n",
    "# Iterate over some gammas to find the best possible values\n",
    "for gamma in gammas:\n",
    "    print(f\"Gamma = {gamma}\")\n",
    "    \n",
    "    # Iterate over all jet numbers dataframes\n",
    "    for i in range(len(features_to_keep)):\n",
    "        initial_w = np.zeros(len(features_to_keep[i]))\n",
    "        y = Y_arr[i]\n",
    "        tx = TX_arr[i]\n",
    "        weights, loss = least_squares_SGD(y, tx, initial_w, batch_size, max_iters, gamma)\n",
    "        print(f\"    For JET_NB {i}, the obtained loss is {loss:>15}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LEAST SQUARES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For JET_NB 0, the obtained loss is 0.3919078212020878\n",
      "For JET_NB 1, the obtained loss is 0.41308946507999145\n",
      "For JET_NB 2, the obtained loss is 0.3549970852299624\n",
      "For JET_NB 3, the obtained loss is 0.43547554636124286\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(features_to_keep)):\n",
    "    initial_w = np.zeros(len(features_to_keep[i]))\n",
    "    y = Y_arr[i]\n",
    "    tx = TX_arr[i]\n",
    "    weights, loss = least_squares(y, tx)\n",
    "    print(f\"For JET_NB {i}, the obtained loss is {loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RIDGE REGRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    min loss for a 2 polynomial expansion feature = 0.7578018775824853\n",
      "    min loss for a 3 polynomial expansion feature = 1.3711918852901765\n",
      "    min loss for a 4 polynomial expansion feature = 95.74535465786285\n",
      "For JET_NB 0, the obtained best degree is 2 and lambda is 0.14873521072935117\n",
      "    min loss for a 2 polynomial expansion feature = 0.8269923159385287\n",
      "    min loss for a 3 polynomial expansion feature = 0.8213430080153667\n",
      "    min loss for a 4 polynomial expansion feature = 0.826324413690626\n",
      "For JET_NB 1, the obtained best degree is 3 and lambda is 0.0001\n",
      "    min loss for a 2 polynomial expansion feature = 0.8077329060928659\n",
      "    min loss for a 3 polynomial expansion feature = 0.7977741061974317\n",
      "    min loss for a 4 polynomial expansion feature = 0.792004171469733\n",
      "For JET_NB 2, the obtained best degree is 4 and lambda is 0.01610262027560939\n",
      "    min loss for a 2 polynomial expansion feature = 0.8190071169100009\n",
      "    min loss for a 3 polynomial expansion feature = 0.8073636374533862\n",
      "    min loss for a 4 polynomial expansion feature = 0.8043028920252104\n",
      "For JET_NB 3, the obtained best degree is 4 and lambda is 0.0001\n",
      "CPU times: user 14min 46s, sys: 29.6 s, total: 15min 16s\n",
      "Wall time: 4min 10s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "seed=15\n",
    "degrees=[2,3,4]\n",
    "k_fold=4\n",
    "lambdas = np.logspace(-4, 0, 30)\n",
    "\n",
    "for i in range(len(features_to_keep)):\n",
    "    y = Y_arr[i]\n",
    "    tx = TX_arr[i]\n",
    "    degree, lambda_ = cross_validation_demo_ridge(y, tx, seed, degrees, k_fold, lambdas)\n",
    "    print(f\"For JET_NB {i}, the obtained best degree is {degree} and lambda is {lambda_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOGISTIC REGRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "max_iters=10000\n",
    "gamma = 0.05\n",
    "\n",
    "for i in range(len(features_to_keep)):\n",
    "    y = Y_arr[i]\n",
    "    tx = TX_arr[i]\n",
    "    weights, loss = logistic_regression(y, tx, max_iters, gamma)\n",
    "    print(f\"For JET_NB {i}, the obtained loss is {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Current iteration=0, loss=51940.29082807893\n",
      "    Current iteration=0, loss=51940.290828078934\n",
      "    Current iteration=0, loss=51940.29082807893\n",
      "    Current iteration=0, loss=51940.290828078934\n",
      "    Current iteration=0, loss=51940.29082807893\n",
      "    Current iteration=0, loss=51940.290828078934\n",
      "    Current iteration=0, loss=51940.29082807893\n",
      "    Current iteration=0, loss=51940.290828078934\n",
      "    Current iteration=0, loss=51940.29082807893\n",
      "    Current iteration=0, loss=51940.290828078934\n",
      "    Current iteration=0, loss=51940.29082807893\n",
      "    Current iteration=0, loss=51940.290828078934\n",
      "    Current iteration=0, loss=51940.29082807893\n",
      "    Current iteration=0, loss=51940.290828078934\n",
      "    Current iteration=0, loss=51940.29082807893\n",
      "    Current iteration=0, loss=51940.290828078934\n",
      "    Current iteration=0, loss=51940.29082807893\n",
      "    Current iteration=0, loss=51940.290828078934\n",
      "    Current iteration=0, loss=51940.29082807893\n",
      "    Current iteration=0, loss=51940.290828078934\n",
      "    Current iteration=0, loss=51940.29082807893\n",
      "    Current iteration=0, loss=51940.290828078934\n",
      "    Current iteration=0, loss=51940.29082807893\n",
      "    Current iteration=0, loss=51940.290828078934\n",
      "##### [195641.6601093234, 255980.45075817168, 442082.8380413326] ####\n",
      "For JET_NB 0, the obtained best degree is 3 and lambda is 0.1 and rsme is 195641.6601093234\n",
      "    Current iteration=0, loss=40312.05372700538\n",
      "    Current iteration=0, loss=40312.05372700538\n",
      "    Current iteration=0, loss=40312.05372700539\n",
      "    Current iteration=0, loss=40312.05372700538\n",
      "    Current iteration=0, loss=40312.05372700538\n",
      "    Current iteration=0, loss=40312.05372700538\n",
      "    Current iteration=0, loss=40312.05372700539\n",
      "    Current iteration=0, loss=40312.05372700538\n",
      "    Current iteration=0, loss=40312.05372700538\n",
      "    Current iteration=0, loss=40312.05372700538\n",
      "    Current iteration=0, loss=40312.05372700539\n",
      "    Current iteration=0, loss=40312.05372700538\n",
      "    Current iteration=0, loss=40312.05372700538\n",
      "    Current iteration=0, loss=40312.05372700538\n",
      "    Current iteration=0, loss=40312.05372700539\n",
      "    Current iteration=0, loss=40312.05372700538\n",
      "    Current iteration=0, loss=40312.05372700538\n",
      "    Current iteration=0, loss=40312.05372700538\n",
      "    Current iteration=0, loss=40312.05372700539\n",
      "    Current iteration=0, loss=40312.05372700538\n",
      "    Current iteration=0, loss=40312.05372700538\n",
      "    Current iteration=0, loss=40312.05372700538\n",
      "    Current iteration=0, loss=40312.05372700539\n",
      "    Current iteration=0, loss=40312.05372700538\n",
      "##### [215660.62943317622, 209130.2950456762, 250631.85620572206] ####\n",
      "For JET_NB 1, the obtained best degree is 4 and lambda is 0.1 and rsme is 209130.2950456762\n",
      "    Current iteration=0, loss=26188.4867759159\n",
      "    Current iteration=0, loss=26188.4867759159\n",
      "    Current iteration=0, loss=26188.4867759159\n",
      "    Current iteration=0, loss=26188.4867759159\n",
      "    Current iteration=0, loss=26188.4867759159\n",
      "    Current iteration=0, loss=26188.4867759159\n",
      "    Current iteration=0, loss=26188.4867759159\n",
      "    Current iteration=0, loss=26188.4867759159\n",
      "    Current iteration=0, loss=26188.4867759159\n",
      "    Current iteration=0, loss=26188.4867759159\n",
      "    Current iteration=0, loss=26188.4867759159\n",
      "    Current iteration=0, loss=26188.4867759159\n",
      "    Current iteration=0, loss=26188.4867759159\n",
      "    Current iteration=0, loss=26188.4867759159\n",
      "    Current iteration=0, loss=26188.4867759159\n",
      "    Current iteration=0, loss=26188.4867759159\n",
      "    Current iteration=0, loss=26188.4867759159\n",
      "    Current iteration=0, loss=26188.4867759159\n",
      "    Current iteration=0, loss=26188.4867759159\n",
      "    Current iteration=0, loss=26188.4867759159\n",
      "    Current iteration=0, loss=26188.4867759159\n",
      "    Current iteration=0, loss=26188.4867759159\n",
      "    Current iteration=0, loss=26188.4867759159\n",
      "    Current iteration=0, loss=26188.4867759159\n",
      "##### [137191.81706105208, 122296.85753562348, 165938.0945793359] ####\n",
      "For JET_NB 2, the obtained best degree is 4 and lambda is 0.1 and rsme is 122296.85753562348\n",
      "    Current iteration=0, loss=11522.185582447979\n",
      "    Current iteration=0, loss=11522.185582447979\n",
      "    Current iteration=0, loss=11522.185582447979\n",
      "    Current iteration=0, loss=11522.185582447979\n",
      "    Current iteration=0, loss=11522.185582447979\n",
      "    Current iteration=0, loss=11522.185582447979\n",
      "    Current iteration=0, loss=11522.185582447979\n",
      "    Current iteration=0, loss=11522.185582447979\n",
      "    Current iteration=0, loss=11522.185582447979\n",
      "    Current iteration=0, loss=11522.185582447979\n",
      "    Current iteration=0, loss=11522.185582447979\n",
      "    Current iteration=0, loss=11522.185582447979\n",
      "    Current iteration=0, loss=11522.185582447979\n",
      "    Current iteration=0, loss=11522.185582447979\n",
      "    Current iteration=0, loss=11522.185582447979\n",
      "    Current iteration=0, loss=11522.185582447979\n",
      "    Current iteration=0, loss=11522.185582447979\n",
      "    Current iteration=0, loss=11522.185582447979\n",
      "    Current iteration=0, loss=11522.185582447979\n",
      "    Current iteration=0, loss=11522.185582447979\n",
      "    Current iteration=0, loss=11522.185582447979\n",
      "    Current iteration=0, loss=11522.185582447979\n",
      "    Current iteration=0, loss=11522.185582447979\n",
      "    Current iteration=0, loss=11522.185582447979\n",
      "##### [73712.29490795336, 62892.20145095332, 70240.37883521136] ####\n",
      "For JET_NB 3, the obtained best degree is 4 and lambda is 0.1 and rsme is 62892.20145095332\n",
      "CPU times: user 33min 28s, sys: 26.4 s, total: 33min 54s\n",
      "Wall time: 8min 40s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "max_iters=1000\n",
    "seed=15\n",
    "degrees=[2, 3, 4, 6, 7]\n",
    "k_fold= 4\n",
    "#gammas = [0.1, 0.01, 0.001, 0.0001]\n",
    "gammas = [0.1, 0.01]\n",
    "\n",
    "\n",
    "\n",
    "for i in range(NUMBER_GROUPS):\n",
    "    y = Y_arr[i]\n",
    "    y[y == - 1.0] = 0.0\n",
    "    tx = TX_arr[i]\n",
    "    degree, gamma, rmse_min = cross_validation_demo_logistic(y, tx, max_iters, seed, degrees, k_fold, gammas)\n",
    "    print(f\"For JET_NB {i}, the obtained best degree is {degree} and gamma is {gamma} and loss is {rmse_min}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Current iteration=0, loss=51940.29082807893\n",
      "    Current iteration=0, loss=51940.290828078934\n",
      "    Current iteration=0, loss=51940.29082807893\n",
      "    Current iteration=0, loss=51940.290828078934\n",
      "    Current iteration=0, loss=51940.29082807893\n",
      "    Current iteration=0, loss=51940.290828078934\n",
      "    Current iteration=0, loss=51940.29082807893\n",
      "    Current iteration=0, loss=51940.290828078934\n",
      "    Current iteration=0, loss=51940.29082807893\n",
      "    Current iteration=0, loss=51940.290828078934\n",
      "    Current iteration=0, loss=51940.29082807893\n",
      "    Current iteration=0, loss=51940.290828078934\n",
      "    Current iteration=0, loss=51940.29082807893\n",
      "    Current iteration=0, loss=51940.290828078934\n",
      "    Current iteration=0, loss=51940.29082807893\n",
      "    Current iteration=0, loss=51940.290828078934\n",
      "    Current iteration=0, loss=51940.29082807893\n",
      "    Current iteration=0, loss=51940.290828078934\n",
      "    Current iteration=0, loss=51940.29082807893\n",
      "    Current iteration=0, loss=51940.290828078934\n",
      "    Current iteration=0, loss=51940.29082807893\n",
      "    Current iteration=0, loss=51940.290828078934\n",
      "    Current iteration=0, loss=51940.29082807893\n",
      "    Current iteration=0, loss=51940.290828078934\n",
      "    Current iteration=0, loss=51940.29082807893\n",
      "    Current iteration=0, loss=51940.290828078934\n",
      "    Current iteration=0, loss=51940.29082807893\n",
      "    Current iteration=0, loss=51940.290828078934\n",
      "    Current iteration=0, loss=51940.29082807893\n",
      "    Current iteration=0, loss=51940.290828078934\n",
      "    Current iteration=0, loss=51940.29082807893\n",
      "    Current iteration=0, loss=51940.290828078934\n",
      "    Current iteration=0, loss=51940.29082807893\n",
      "    Current iteration=0, loss=51940.290828078934\n",
      "    Current iteration=0, loss=51940.29082807893\n",
      "    Current iteration=0, loss=51940.290828078934\n",
      "    Current iteration=0, loss=51940.29082807893\n",
      "    Current iteration=0, loss=51940.290828078934\n",
      "    Current iteration=0, loss=51940.29082807893\n",
      "    Current iteration=0, loss=51940.290828078934\n",
      "    Current iteration=0, loss=51940.29082807893\n",
      "    Current iteration=0, loss=51940.290828078934\n",
      "    Current iteration=0, loss=51940.29082807893\n",
      "    Current iteration=0, loss=51940.290828078934\n",
      "    Current iteration=0, loss=51940.29082807893\n",
      "    Current iteration=0, loss=51940.290828078934\n",
      "    Current iteration=0, loss=51940.29082807893\n",
      "    Current iteration=0, loss=51940.290828078934\n",
      "    Current iteration=0, loss=51940.29082807893\n",
      "    Current iteration=0, loss=51940.290828078934\n",
      "    Current iteration=0, loss=51940.29082807893\n",
      "    Current iteration=0, loss=51940.290828078934\n",
      "    Current iteration=0, loss=51940.29082807893\n",
      "    Current iteration=0, loss=51940.290828078934\n",
      "    Current iteration=0, loss=51940.29082807893\n",
      "    Current iteration=0, loss=51940.290828078934\n",
      "    Current iteration=0, loss=51940.29082807893\n",
      "    Current iteration=0, loss=51940.290828078934\n",
      "    Current iteration=0, loss=51940.29082807893\n",
      "    Current iteration=0, loss=51940.290828078934\n",
      "##### [213423.43831948875, 195641.6601093234, 255980.45075817168, 442082.8380413326, 405453.1558725719] ####\n",
      "For JET_NB 0, the obtained best degree is 3 and gamma is 0.1 and loss is 195641.6601093234\n",
      "    Current iteration=0, loss=40312.05372700538\n",
      "    Current iteration=0, loss=40312.05372700538\n",
      "    Current iteration=0, loss=40312.05372700539\n",
      "    Current iteration=0, loss=40312.05372700538\n",
      "    Current iteration=0, loss=40312.05372700538\n",
      "    Current iteration=0, loss=40312.05372700538\n",
      "    Current iteration=0, loss=40312.05372700539\n",
      "    Current iteration=0, loss=40312.05372700538\n",
      "    Current iteration=0, loss=40312.05372700538\n",
      "    Current iteration=0, loss=40312.05372700538\n",
      "    Current iteration=0, loss=40312.05372700539\n",
      "    Current iteration=0, loss=40312.05372700538\n",
      "    Current iteration=0, loss=40312.05372700538\n",
      "    Current iteration=0, loss=40312.05372700538\n",
      "    Current iteration=0, loss=40312.05372700539\n",
      "    Current iteration=0, loss=40312.05372700538\n",
      "    Current iteration=0, loss=40312.05372700538\n",
      "    Current iteration=0, loss=40312.05372700538\n",
      "    Current iteration=0, loss=40312.05372700539\n",
      "    Current iteration=0, loss=40312.05372700538\n",
      "    Current iteration=0, loss=40312.05372700538\n",
      "    Current iteration=0, loss=40312.05372700538\n",
      "    Current iteration=0, loss=40312.05372700539\n",
      "    Current iteration=0, loss=40312.05372700538\n",
      "    Current iteration=0, loss=40312.05372700538\n",
      "    Current iteration=0, loss=40312.05372700538\n",
      "    Current iteration=0, loss=40312.05372700539\n",
      "    Current iteration=0, loss=40312.05372700538\n",
      "    Current iteration=0, loss=40312.05372700538\n",
      "    Current iteration=0, loss=40312.05372700538\n",
      "    Current iteration=0, loss=40312.05372700539\n",
      "    Current iteration=0, loss=40312.05372700538\n",
      "    Current iteration=0, loss=40312.05372700538\n",
      "    Current iteration=0, loss=40312.05372700538\n",
      "    Current iteration=0, loss=40312.05372700539\n",
      "    Current iteration=0, loss=40312.05372700538\n",
      "    Current iteration=0, loss=40312.05372700538\n",
      "    Current iteration=0, loss=40312.05372700538\n",
      "    Current iteration=0, loss=40312.05372700539\n",
      "    Current iteration=0, loss=40312.05372700538\n",
      "    Current iteration=0, loss=40312.05372700538\n",
      "    Current iteration=0, loss=40312.05372700538\n",
      "    Current iteration=0, loss=40312.05372700539\n",
      "    Current iteration=0, loss=40312.05372700538\n",
      "    Current iteration=0, loss=40312.05372700538\n",
      "    Current iteration=0, loss=40312.05372700538\n",
      "    Current iteration=0, loss=40312.05372700539\n",
      "    Current iteration=0, loss=40312.05372700538\n",
      "    Current iteration=0, loss=40312.05372700538\n",
      "    Current iteration=0, loss=40312.05372700538\n",
      "    Current iteration=0, loss=40312.05372700539\n",
      "    Current iteration=0, loss=40312.05372700538\n",
      "    Current iteration=0, loss=40312.05372700538\n",
      "    Current iteration=0, loss=40312.05372700538\n",
      "    Current iteration=0, loss=40312.05372700539\n",
      "    Current iteration=0, loss=40312.05372700538\n",
      "    Current iteration=0, loss=40312.05372700538\n",
      "    Current iteration=0, loss=40312.05372700538\n",
      "    Current iteration=0, loss=40312.05372700539\n",
      "    Current iteration=0, loss=40312.05372700538\n",
      "##### [196652.35964137124, 215660.62943317622, 209130.2950456762, 250631.85620572206, 298113.48009384365] ####\n",
      "For JET_NB 1, the obtained best degree is 2 and gamma is 0.001 and loss is 196652.35964137124\n",
      "    Current iteration=0, loss=26188.4867759159\n",
      "    Current iteration=0, loss=26188.4867759159\n",
      "    Current iteration=0, loss=26188.4867759159\n",
      "    Current iteration=0, loss=26188.4867759159\n",
      "    Current iteration=0, loss=26188.4867759159\n",
      "    Current iteration=0, loss=26188.4867759159\n",
      "    Current iteration=0, loss=26188.4867759159\n",
      "    Current iteration=0, loss=26188.4867759159\n",
      "    Current iteration=0, loss=26188.4867759159\n",
      "    Current iteration=0, loss=26188.4867759159\n",
      "    Current iteration=0, loss=26188.4867759159\n",
      "    Current iteration=0, loss=26188.4867759159\n",
      "    Current iteration=0, loss=26188.4867759159\n",
      "    Current iteration=0, loss=26188.4867759159\n",
      "    Current iteration=0, loss=26188.4867759159\n",
      "    Current iteration=0, loss=26188.4867759159\n",
      "    Current iteration=0, loss=26188.4867759159\n",
      "    Current iteration=0, loss=26188.4867759159\n",
      "    Current iteration=0, loss=26188.4867759159\n",
      "    Current iteration=0, loss=26188.4867759159\n",
      "    Current iteration=0, loss=26188.4867759159\n",
      "    Current iteration=0, loss=26188.4867759159\n",
      "    Current iteration=0, loss=26188.4867759159\n",
      "    Current iteration=0, loss=26188.4867759159\n",
      "    Current iteration=0, loss=26188.4867759159\n",
      "    Current iteration=0, loss=26188.4867759159\n",
      "    Current iteration=0, loss=26188.4867759159\n",
      "    Current iteration=0, loss=26188.4867759159\n",
      "    Current iteration=0, loss=26188.4867759159\n",
      "    Current iteration=0, loss=26188.4867759159\n",
      "    Current iteration=0, loss=26188.4867759159\n",
      "    Current iteration=0, loss=26188.4867759159\n",
      "    Current iteration=0, loss=26188.4867759159\n",
      "    Current iteration=0, loss=26188.4867759159\n",
      "    Current iteration=0, loss=26188.4867759159\n",
      "    Current iteration=0, loss=26188.4867759159\n",
      "    Current iteration=0, loss=26188.4867759159\n",
      "    Current iteration=0, loss=26188.4867759159\n",
      "    Current iteration=0, loss=26188.4867759159\n",
      "    Current iteration=0, loss=26188.4867759159\n",
      "    Current iteration=0, loss=26188.4867759159\n",
      "    Current iteration=0, loss=26188.4867759159\n",
      "    Current iteration=0, loss=26188.4867759159\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Current iteration=0, loss=26188.4867759159\n",
      "    Current iteration=0, loss=26188.4867759159\n",
      "    Current iteration=0, loss=26188.4867759159\n",
      "    Current iteration=0, loss=26188.4867759159\n",
      "    Current iteration=0, loss=26188.4867759159\n",
      "    Current iteration=0, loss=26188.4867759159\n",
      "    Current iteration=0, loss=26188.4867759159\n",
      "    Current iteration=0, loss=26188.4867759159\n",
      "    Current iteration=0, loss=26188.4867759159\n",
      "    Current iteration=0, loss=26188.4867759159\n",
      "    Current iteration=0, loss=26188.4867759159\n",
      "    Current iteration=0, loss=26188.4867759159\n",
      "    Current iteration=0, loss=26188.4867759159\n",
      "    Current iteration=0, loss=26188.4867759159\n",
      "    Current iteration=0, loss=26188.4867759159\n",
      "    Current iteration=0, loss=26188.4867759159\n",
      "    Current iteration=0, loss=26188.4867759159\n",
      "##### [203835.0045943192, 137191.81706105208, 122296.85753562348, 165938.0945793359, 189362.35050772002] ####\n",
      "For JET_NB 2, the obtained best degree is 4 and gamma is 0.1 and loss is 122296.85753562348\n",
      "    Current iteration=0, loss=11522.185582447979\n",
      "    Current iteration=0, loss=11522.185582447979\n",
      "    Current iteration=0, loss=11522.185582447979\n",
      "    Current iteration=0, loss=11522.185582447979\n",
      "    Current iteration=0, loss=11522.185582447979\n",
      "    Current iteration=0, loss=11522.185582447979\n",
      "    Current iteration=0, loss=11522.185582447979\n",
      "    Current iteration=0, loss=11522.185582447979\n",
      "    Current iteration=0, loss=11522.185582447979\n",
      "    Current iteration=0, loss=11522.185582447979\n",
      "    Current iteration=0, loss=11522.185582447979\n",
      "    Current iteration=0, loss=11522.185582447979\n",
      "    Current iteration=0, loss=11522.185582447979\n",
      "    Current iteration=0, loss=11522.185582447979\n",
      "    Current iteration=0, loss=11522.185582447979\n",
      "    Current iteration=0, loss=11522.185582447979\n",
      "    Current iteration=0, loss=11522.185582447979\n",
      "    Current iteration=0, loss=11522.185582447979\n",
      "    Current iteration=0, loss=11522.185582447979\n",
      "    Current iteration=0, loss=11522.185582447979\n",
      "    Current iteration=0, loss=11522.185582447979\n",
      "    Current iteration=0, loss=11522.185582447979\n",
      "    Current iteration=0, loss=11522.185582447979\n",
      "    Current iteration=0, loss=11522.185582447979\n",
      "    Current iteration=0, loss=11522.185582447979\n",
      "    Current iteration=0, loss=11522.185582447979\n",
      "    Current iteration=0, loss=11522.185582447979\n",
      "    Current iteration=0, loss=11522.185582447979\n",
      "    Current iteration=0, loss=11522.185582447979\n",
      "    Current iteration=0, loss=11522.185582447979\n",
      "    Current iteration=0, loss=11522.185582447979\n",
      "    Current iteration=0, loss=11522.185582447979\n",
      "    Current iteration=0, loss=11522.185582447979\n",
      "    Current iteration=0, loss=11522.185582447979\n",
      "    Current iteration=0, loss=11522.185582447979\n",
      "    Current iteration=0, loss=11522.185582447979\n",
      "    Current iteration=0, loss=11522.185582447979\n",
      "    Current iteration=0, loss=11522.185582447979\n",
      "    Current iteration=0, loss=11522.185582447979\n",
      "    Current iteration=0, loss=11522.185582447979\n",
      "    Current iteration=0, loss=11522.185582447979\n",
      "    Current iteration=0, loss=11522.185582447979\n",
      "    Current iteration=0, loss=11522.185582447979\n",
      "    Current iteration=0, loss=11522.185582447979\n",
      "    Current iteration=0, loss=11522.185582447979\n",
      "    Current iteration=0, loss=11522.185582447979\n",
      "    Current iteration=0, loss=11522.185582447979\n",
      "    Current iteration=0, loss=11522.185582447979\n",
      "    Current iteration=0, loss=11522.185582447979\n",
      "    Current iteration=0, loss=11522.185582447979\n",
      "    Current iteration=0, loss=11522.185582447979\n",
      "    Current iteration=0, loss=11522.185582447979\n",
      "    Current iteration=0, loss=11522.185582447979\n",
      "    Current iteration=0, loss=11522.185582447979\n",
      "    Current iteration=0, loss=11522.185582447979\n",
      "    Current iteration=0, loss=11522.185582447979\n",
      "    Current iteration=0, loss=11522.185582447979\n",
      "    Current iteration=0, loss=11522.185582447979\n",
      "    Current iteration=0, loss=11522.185582447979\n",
      "    Current iteration=0, loss=11522.185582447979\n",
      "##### [67445.78306234139, 73712.29490795336, 62892.20145095332, 70240.37883521136, 81428.64720186661] ####\n",
      "For JET_NB 3, the obtained best degree is 4 and gamma is 0.1 and loss is 62892.20145095332\n",
      "CPU times: user 1h 22min 12s, sys: 57 s, total: 1h 23min 9s\n",
      "Wall time: 21min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "max_iters = 1_000\n",
    "seed = 15\n",
    "degrees = [2, 3, 4, 6, 7]\n",
    "k_fold = 4\n",
    "#gammas = [0.1, 0.01, 0.001, 0.0001]\n",
    "gammas = [0.1, 0.01, 0.001]\n",
    "\n",
    "\n",
    "\n",
    "for i in range(NUMBER_GROUPS):\n",
    "    y = Y_arr[i]\n",
    "    y[y == - 1.0] = 0.0\n",
    "    tx = TX_arr[i]\n",
    "    degree, gamma, rmse_min = cross_validation_demo_logistic(y, tx, max_iters, seed, degrees, k_fold, gammas)\n",
    "    print(f\"For JET_NB {i}, the obtained best degree is {degree} and gamma is {gamma} and loss is {rmse_min}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Current iteration=0, loss=51940.29082807893\n",
      "    Current iteration=1000, loss=1701921.215343946\n",
      "    Current iteration=0, loss=51940.290828078934\n",
      "    Current iteration=1000, loss=1695075.832470696\n",
      "    Current iteration=0, loss=51940.29082807893\n",
      "    Current iteration=1000, loss=1653888.7238033875\n",
      "    Current iteration=0, loss=51940.290828078934\n",
      "    Current iteration=1000, loss=1753292.2038408339\n",
      "    Current iteration=0, loss=51940.29082807893\n",
      "    Current iteration=1000, loss=2239571.580951998\n",
      "    Current iteration=0, loss=51940.290828078934\n",
      "    Current iteration=1000, loss=2117023.4772716025\n",
      "    Current iteration=0, loss=51940.29082807893\n",
      "    Current iteration=1000, loss=2117828.3568179896\n",
      "    Current iteration=0, loss=51940.290828078934\n",
      "    Current iteration=1000, loss=2495089.3751767017\n",
      "    Current iteration=0, loss=51940.29082807893\n",
      "    Current iteration=1000, loss=256034.64578245668\n",
      "    Current iteration=0, loss=51940.290828078934\n",
      "    Current iteration=1000, loss=242313.0676791588\n",
      "    Current iteration=0, loss=51940.29082807893\n",
      "    Current iteration=1000, loss=241787.28249639072\n",
      "    Current iteration=0, loss=51940.290828078934\n",
      "    Current iteration=1000, loss=284515.89736310596\n",
      "    Current iteration=0, loss=51940.29082807893\n",
      "    Current iteration=1000, loss=646524.7082171553\n",
      "    Current iteration=0, loss=51940.290828078934\n",
      "    Current iteration=1000, loss=628733.4134015737\n",
      "    Current iteration=0, loss=51940.29082807893\n",
      "    Current iteration=1000, loss=590854.4030617764\n",
      "    Current iteration=0, loss=51940.290828078934\n",
      "    Current iteration=1000, loss=650877.3385003234\n",
      "    Current iteration=0, loss=51940.29082807893\n",
      "    Current iteration=1000, loss=963849.0593103301\n",
      "    Current iteration=0, loss=51940.290828078934\n",
      "    Current iteration=1000, loss=997689.3394732906\n",
      "    Current iteration=0, loss=51940.29082807893\n",
      "    Current iteration=1000, loss=1050257.0071230559\n",
      "    Current iteration=0, loss=51940.290828078934\n",
      "    Current iteration=1000, loss=955326.1643592978\n",
      "    Current iteration=0, loss=51940.29082807893\n",
      "    Current iteration=1000, loss=2295370.9966332237\n",
      "    Current iteration=0, loss=51940.290828078934\n",
      "    Current iteration=1000, loss=562496.0952304469\n",
      "    Current iteration=0, loss=51940.29082807893\n",
      "    Current iteration=1000, loss=573210.8982885241\n",
      "    Current iteration=0, loss=51940.290828078934\n",
      "    Current iteration=1000, loss=566525.2918058471\n",
      "    Current iteration=0, loss=51940.29082807893\n",
      "    Current iteration=1000, loss=595951.2149477184\n",
      "    Current iteration=0, loss=51940.290828078934\n",
      "    Current iteration=1000, loss=544000.9444273068\n",
      "    Current iteration=0, loss=51940.29082807893\n",
      "    Current iteration=1000, loss=600106.6542545413\n",
      "    Current iteration=0, loss=51940.290828078934\n",
      "    Current iteration=1000, loss=614540.884819394\n",
      "    Current iteration=0, loss=51940.29082807893\n",
      "    Current iteration=1000, loss=939462.6422210652\n",
      "    Current iteration=0, loss=51940.290828078934\n",
      "    Current iteration=1000, loss=1054472.195282323\n",
      "    Current iteration=0, loss=51940.29082807893\n",
      "    Current iteration=1000, loss=1014722.8289160402\n",
      "    Current iteration=0, loss=51940.290828078934\n",
      "    Current iteration=1000, loss=922579.771243283\n",
      "    Current iteration=0, loss=51940.29082807893\n",
      "    Current iteration=1000, loss=1444182.891942617\n",
      "    Current iteration=0, loss=51940.290828078934\n",
      "    Current iteration=1000, loss=1027293.7480149843\n",
      "    Current iteration=0, loss=51940.29082807893\n",
      "    Current iteration=1000, loss=2220620.345090318\n",
      "    Current iteration=0, loss=51940.290828078934\n",
      "    Current iteration=1000, loss=2675198.886859429\n",
      "    Current iteration=0, loss=51940.29082807893\n",
      "    Current iteration=1000, loss=636543.2086070168\n",
      "    Current iteration=0, loss=51940.290828078934\n",
      "    Current iteration=0, loss=51940.29082807893\n",
      "    Current iteration=1000, loss=801723.5507616228\n",
      "    Current iteration=0, loss=51940.290828078934\n",
      "    Current iteration=0, loss=51940.29082807893\n",
      "    Current iteration=1000, loss=667548.6566493828\n",
      "    Current iteration=0, loss=51940.290828078934\n",
      "    Current iteration=1000, loss=805292.6399837657\n",
      "    Current iteration=0, loss=51940.29082807893\n",
      "    Current iteration=1000, loss=801723.5507616228\n",
      "    Current iteration=0, loss=51940.290828078934\n",
      "    Current iteration=1000, loss=794936.8553135874\n",
      "    Current iteration=0, loss=51940.29082807893\n",
      "    Current iteration=1000, loss=962729.5554695723\n",
      "    Current iteration=0, loss=51940.290828078934\n",
      "    Current iteration=1000, loss=811414.1353014978\n",
      "    Current iteration=0, loss=51940.29082807893\n",
      "    Current iteration=1000, loss=808740.4407437005\n",
      "    Current iteration=0, loss=51940.290828078934\n",
      "    Current iteration=1000, loss=802707.2771800326\n",
      "    Current iteration=0, loss=51940.29082807893\n",
      "    Current iteration=1000, loss=714280.5095195265\n",
      "    Current iteration=0, loss=51940.290828078934\n",
      "    Current iteration=0, loss=51940.29082807893\n",
      "    Current iteration=0, loss=51940.290828078934\n",
      "    Current iteration=0, loss=51940.29082807893\n",
      "    Current iteration=1000, loss=712669.6114978223\n",
      "    Current iteration=0, loss=51940.290828078934\n",
      "    Current iteration=0, loss=51940.29082807893\n",
      "    Current iteration=0, loss=51940.290828078934\n",
      "    Current iteration=0, loss=51940.29082807893\n",
      "    Current iteration=1000, loss=739988.185963525\n",
      "    Current iteration=0, loss=51940.290828078934\n",
      "    Current iteration=0, loss=51940.29082807893\n",
      "    Current iteration=0, loss=51940.290828078934\n",
      "##### [85246.76174882308, 224686.61376075388, 209115.11521479837, 225207.97165001312, 354814.68805976684] ####\n",
      "For JET_NB 0, the obtained best degree is 1 and gamma is 0.001 and loss is 85246.76174882308\n",
      "    Current iteration=0, loss=40312.05372700538\n",
      "    Current iteration=1000, loss=1369487.7851222556\n",
      "    Current iteration=0, loss=40312.05372700538\n",
      "    Current iteration=1000, loss=1347891.0505915852\n",
      "    Current iteration=0, loss=40312.05372700539\n",
      "    Current iteration=1000, loss=1342736.0837677608\n",
      "    Current iteration=0, loss=40312.05372700538\n",
      "    Current iteration=1000, loss=1352123.6903983953\n",
      "    Current iteration=0, loss=40312.05372700538\n",
      "    Current iteration=1000, loss=1650494.1447669133\n",
      "    Current iteration=0, loss=40312.05372700538\n",
      "    Current iteration=1000, loss=1563780.3535540416\n",
      "    Current iteration=0, loss=40312.05372700539\n",
      "    Current iteration=1000, loss=1611766.7628395744\n",
      "    Current iteration=0, loss=40312.05372700538\n",
      "    Current iteration=1000, loss=1540956.469796457\n",
      "    Current iteration=0, loss=40312.05372700538\n",
      "    Current iteration=1000, loss=334259.6260590068\n",
      "    Current iteration=0, loss=40312.05372700538\n",
      "    Current iteration=1000, loss=317781.5610569813\n",
      "    Current iteration=0, loss=40312.05372700539\n",
      "    Current iteration=1000, loss=327423.9547250663\n",
      "    Current iteration=0, loss=40312.05372700538\n",
      "    Current iteration=1000, loss=311382.88896897767\n",
      "    Current iteration=0, loss=40312.05372700538\n",
      "    Current iteration=1000, loss=580818.2726423708\n",
      "    Current iteration=0, loss=40312.05372700538\n",
      "    Current iteration=1000, loss=608445.829117665\n",
      "    Current iteration=0, loss=40312.05372700539\n",
      "    Current iteration=1000, loss=614166.8046782139\n",
      "    Current iteration=0, loss=40312.05372700538\n",
      "    Current iteration=1000, loss=610587.0236717177\n",
      "    Current iteration=0, loss=40312.05372700538\n",
      "    Current iteration=1000, loss=932735.9795897536\n",
      "    Current iteration=0, loss=40312.05372700538\n",
      "    Current iteration=1000, loss=780404.9977950417\n",
      "    Current iteration=0, loss=40312.05372700539\n",
      "    Current iteration=1000, loss=784559.6173111114\n",
      "    Current iteration=0, loss=40312.05372700538\n",
      "    Current iteration=1000, loss=795264.1111100242\n",
      "    Current iteration=0, loss=40312.05372700538\n",
      "    Current iteration=1000, loss=566492.0945835067\n",
      "    Current iteration=0, loss=40312.05372700538\n",
      "    Current iteration=1000, loss=599843.7998908691\n",
      "    Current iteration=0, loss=40312.05372700539\n",
      "    Current iteration=1000, loss=601179.0212136514\n",
      "    Current iteration=0, loss=40312.05372700538\n",
      "    Current iteration=1000, loss=598194.1939136995\n",
      "    Current iteration=0, loss=40312.05372700538\n",
      "    Current iteration=1000, loss=659205.0427128831\n",
      "    Current iteration=0, loss=40312.05372700538\n",
      "    Current iteration=1000, loss=634229.4574373916\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Current iteration=0, loss=40312.05372700539\n",
      "    Current iteration=1000, loss=652740.829978172\n",
      "    Current iteration=0, loss=40312.05372700538\n",
      "    Current iteration=1000, loss=691387.5883490767\n",
      "    Current iteration=0, loss=40312.05372700538\n",
      "    Current iteration=1000, loss=1229522.2420215467\n",
      "    Current iteration=0, loss=40312.05372700538\n",
      "    Current iteration=1000, loss=1229860.5972564286\n",
      "    Current iteration=0, loss=40312.05372700539\n",
      "    Current iteration=1000, loss=1194485.4433563568\n",
      "    Current iteration=0, loss=40312.05372700538\n",
      "    Current iteration=1000, loss=1169533.1558258631\n",
      "    Current iteration=0, loss=40312.05372700538\n",
      "    Current iteration=1000, loss=1178967.870708149\n",
      "    Current iteration=0, loss=40312.05372700538\n",
      "    Current iteration=1000, loss=1266437.6969828985\n",
      "    Current iteration=0, loss=40312.05372700539\n",
      "    Current iteration=1000, loss=1217033.144045752\n",
      "    Current iteration=0, loss=40312.05372700538\n",
      "    Current iteration=1000, loss=1413897.7162553037\n",
      "    Current iteration=0, loss=40312.05372700538\n",
      "    Current iteration=1000, loss=656881.6633835831\n",
      "    Current iteration=0, loss=40312.05372700538\n",
      "    Current iteration=1000, loss=585449.3250897985\n",
      "    Current iteration=0, loss=40312.05372700539\n",
      "    Current iteration=1000, loss=683984.6319460019\n",
      "    Current iteration=0, loss=40312.05372700538\n",
      "    Current iteration=1000, loss=588954.0374731198\n",
      "    Current iteration=0, loss=40312.05372700538\n",
      "    Current iteration=1000, loss=680877.5963621639\n",
      "    Current iteration=0, loss=40312.05372700538\n",
      "    Current iteration=1000, loss=682568.9752630809\n",
      "    Current iteration=0, loss=40312.05372700539\n",
      "    Current iteration=1000, loss=631438.2484643923\n",
      "    Current iteration=0, loss=40312.05372700538\n",
      "    Current iteration=1000, loss=676249.6134320967\n",
      "    Current iteration=0, loss=40312.05372700538\n",
      "    Current iteration=1000, loss=1419566.540158338\n",
      "    Current iteration=0, loss=40312.05372700538\n",
      "    Current iteration=1000, loss=857188.5633591011\n",
      "    Current iteration=0, loss=40312.05372700539\n",
      "    Current iteration=1000, loss=926990.6298544521\n",
      "    Current iteration=0, loss=40312.05372700538\n",
      "    Current iteration=1000, loss=1965440.6383179259\n",
      "    Current iteration=0, loss=40312.05372700538\n",
      "    Current iteration=1000, loss=722978.9109164721\n",
      "    Current iteration=0, loss=40312.05372700538\n",
      "    Current iteration=1000, loss=693765.625891984\n",
      "    Current iteration=0, loss=40312.05372700539\n",
      "    Current iteration=1000, loss=704176.6898292021\n",
      "    Current iteration=0, loss=40312.05372700538\n",
      "    Current iteration=1000, loss=645786.1707376754\n",
      "    Current iteration=0, loss=40312.05372700538\n",
      "    Current iteration=1000, loss=759789.3712696759\n",
      "    Current iteration=0, loss=40312.05372700538\n",
      "    Current iteration=1000, loss=724589.0825201423\n",
      "    Current iteration=0, loss=40312.05372700539\n",
      "    Current iteration=1000, loss=785000.6540864023\n",
      "    Current iteration=0, loss=40312.05372700538\n",
      "    Current iteration=1000, loss=704019.5228727439\n",
      "    Current iteration=0, loss=40312.05372700538\n",
      "    Current iteration=1000, loss=785120.9394539634\n",
      "    Current iteration=0, loss=40312.05372700538\n",
      "    Current iteration=1000, loss=802787.3481690506\n",
      "    Current iteration=0, loss=40312.05372700539\n",
      "    Current iteration=1000, loss=786104.1078775232\n",
      "    Current iteration=0, loss=40312.05372700538\n",
      "    Current iteration=1000, loss=761831.2942471802\n",
      "##### [107838.61906309938, 207662.11575656437, 226756.40772452595, 194595.5750519198, 221798.0204004026] ####\n",
      "For JET_NB 1, the obtained best degree is 1 and gamma is 0.001 and loss is 107838.61906309938\n",
      "    Current iteration=0, loss=26188.4867759159\n",
      "    Current iteration=1000, loss=1149832.9824502505\n",
      "    Current iteration=0, loss=26188.4867759159\n",
      "    Current iteration=1000, loss=1205738.3088721521\n",
      "    Current iteration=0, loss=26188.4867759159\n",
      "    Current iteration=1000, loss=881234.2076028321\n",
      "    Current iteration=0, loss=26188.4867759159\n",
      "    Current iteration=1000, loss=887249.8286846041\n",
      "    Current iteration=0, loss=26188.4867759159\n",
      "    Current iteration=1000, loss=1237179.7893116684\n",
      "    Current iteration=0, loss=26188.4867759159\n",
      "    Current iteration=1000, loss=757410.4684534997\n",
      "    Current iteration=0, loss=26188.4867759159\n",
      "    Current iteration=1000, loss=496962.2130577858\n",
      "    Current iteration=0, loss=26188.4867759159\n",
      "    Current iteration=1000, loss=588698.0453044565\n",
      "    Current iteration=0, loss=26188.4867759159\n",
      "    Current iteration=1000, loss=350502.39646840247\n",
      "    Current iteration=0, loss=26188.4867759159\n",
      "    Current iteration=1000, loss=350479.429376964\n",
      "    Current iteration=0, loss=26188.4867759159\n",
      "    Current iteration=1000, loss=97103.98109721325\n",
      "    Current iteration=0, loss=26188.4867759159\n",
      "    Current iteration=1000, loss=343062.67636849347\n",
      "    Current iteration=0, loss=26188.4867759159\n",
      "    Current iteration=1000, loss=615411.842452457\n",
      "    Current iteration=0, loss=26188.4867759159\n",
      "    Current iteration=1000, loss=625073.8973285545\n",
      "    Current iteration=0, loss=26188.4867759159\n",
      "    Current iteration=1000, loss=612383.0759726531\n",
      "    Current iteration=0, loss=26188.4867759159\n",
      "    Current iteration=1000, loss=606903.2014040736\n",
      "    Current iteration=0, loss=26188.4867759159\n",
      "    Current iteration=1000, loss=1661535.8423163304\n",
      "    Current iteration=0, loss=26188.4867759159\n",
      "    Current iteration=1000, loss=1661292.0966499883\n",
      "    Current iteration=0, loss=26188.4867759159\n",
      "    Current iteration=1000, loss=1692471.290102035\n",
      "    Current iteration=0, loss=26188.4867759159\n",
      "    Current iteration=1000, loss=1677516.1570538173\n",
      "    Current iteration=0, loss=26188.4867759159\n",
      "    Current iteration=1000, loss=1007188.5836209505\n",
      "    Current iteration=0, loss=26188.4867759159\n",
      "    Current iteration=1000, loss=1089227.9038405688\n",
      "    Current iteration=0, loss=26188.4867759159\n",
      "    Current iteration=1000, loss=1113003.7742836883\n",
      "    Current iteration=0, loss=26188.4867759159\n",
      "    Current iteration=1000, loss=1047924.0765379808\n",
      "    Current iteration=0, loss=26188.4867759159\n",
      "    Current iteration=1000, loss=388140.987257405\n",
      "    Current iteration=0, loss=26188.4867759159\n",
      "    Current iteration=1000, loss=408270.17266087484\n",
      "    Current iteration=0, loss=26188.4867759159\n",
      "    Current iteration=1000, loss=421285.8434082506\n",
      "    Current iteration=0, loss=26188.4867759159\n",
      "    Current iteration=1000, loss=433015.00972516806\n",
      "    Current iteration=0, loss=26188.4867759159\n",
      "    Current iteration=1000, loss=669402.0923093054\n",
      "    Current iteration=0, loss=26188.4867759159\n",
      "    Current iteration=1000, loss=923723.8677131215\n",
      "    Current iteration=0, loss=26188.4867759159\n",
      "    Current iteration=1000, loss=641600.3638100689\n",
      "    Current iteration=0, loss=26188.4867759159\n",
      "    Current iteration=1000, loss=763325.5465986782\n",
      "    Current iteration=0, loss=26188.4867759159\n",
      "    Current iteration=1000, loss=949053.6136330597\n",
      "    Current iteration=0, loss=26188.4867759159\n",
      "    Current iteration=1000, loss=753218.6502614741\n",
      "    Current iteration=0, loss=26188.4867759159\n",
      "    Current iteration=1000, loss=715763.3465630899\n",
      "    Current iteration=0, loss=26188.4867759159\n",
      "    Current iteration=1000, loss=686937.4022658269\n",
      "    Current iteration=0, loss=26188.4867759159\n",
      "    Current iteration=1000, loss=334518.9263679867\n",
      "    Current iteration=0, loss=26188.4867759159\n",
      "    Current iteration=1000, loss=379991.3321672862\n",
      "    Current iteration=0, loss=26188.4867759159\n",
      "    Current iteration=1000, loss=323688.23147149605\n",
      "    Current iteration=0, loss=26188.4867759159\n",
      "    Current iteration=1000, loss=413465.7683849386\n",
      "    Current iteration=0, loss=26188.4867759159\n",
      "    Current iteration=1000, loss=404925.21220274904\n",
      "    Current iteration=0, loss=26188.4867759159\n",
      "    Current iteration=1000, loss=379627.76103924296\n",
      "    Current iteration=0, loss=26188.4867759159\n",
      "    Current iteration=1000, loss=405814.0828410948\n",
      "    Current iteration=0, loss=26188.4867759159\n",
      "    Current iteration=1000, loss=430577.3147036896\n",
      "    Current iteration=0, loss=26188.4867759159\n",
      "    Current iteration=1000, loss=644827.3567015168\n",
      "    Current iteration=0, loss=26188.4867759159\n",
      "    Current iteration=1000, loss=1342302.7045783391\n",
      "    Current iteration=0, loss=26188.4867759159\n",
      "    Current iteration=1000, loss=1209545.2332379739\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Current iteration=0, loss=26188.4867759159\n",
      "    Current iteration=1000, loss=631156.6539103761\n",
      "    Current iteration=0, loss=26188.4867759159\n",
      "    Current iteration=1000, loss=386132.7912588014\n",
      "    Current iteration=0, loss=26188.4867759159\n",
      "    Current iteration=1000, loss=427599.2428709139\n",
      "    Current iteration=0, loss=26188.4867759159\n",
      "    Current iteration=1000, loss=458461.991445715\n",
      "    Current iteration=0, loss=26188.4867759159\n",
      "    Current iteration=1000, loss=446510.8993816383\n",
      "    Current iteration=0, loss=26188.4867759159\n",
      "    Current iteration=1000, loss=426501.43623089866\n",
      "    Current iteration=0, loss=26188.4867759159\n",
      "    Current iteration=1000, loss=443876.15432068123\n",
      "    Current iteration=0, loss=26188.4867759159\n",
      "    Current iteration=1000, loss=492639.6981381142\n",
      "    Current iteration=0, loss=26188.4867759159\n",
      "    Current iteration=1000, loss=446863.2259884725\n",
      "    Current iteration=0, loss=26188.4867759159\n",
      "    Current iteration=1000, loss=549653.152815496\n",
      "    Current iteration=0, loss=26188.4867759159\n",
      "    Current iteration=1000, loss=515974.79069301195\n",
      "    Current iteration=0, loss=26188.4867759159\n",
      "    Current iteration=1000, loss=575134.8144129846\n",
      "    Current iteration=0, loss=26188.4867759159\n",
      "    Current iteration=1000, loss=566312.2452855413\n",
      "##### [73346.32344675701, 201049.33698432174, 127830.10394478735, 124971.15335186658, 142638.65640915782] ####\n",
      "For JET_NB 2, the obtained best degree is 1 and gamma is 0.001 and loss is 73346.32344675701\n",
      "    Current iteration=0, loss=11522.185582447979\n",
      "    Current iteration=1000, loss=492113.6369344669\n",
      "    Current iteration=0, loss=11522.185582447979\n",
      "    Current iteration=1000, loss=498424.5235657785\n",
      "    Current iteration=0, loss=11522.185582447979\n",
      "    Current iteration=1000, loss=493385.2261248555\n",
      "    Current iteration=0, loss=11522.185582447979\n",
      "    Current iteration=1000, loss=504280.3334505009\n",
      "    Current iteration=0, loss=11522.185582447979\n",
      "    Current iteration=1000, loss=396928.0657620281\n",
      "    Current iteration=0, loss=11522.185582447979\n",
      "    Current iteration=1000, loss=401059.63395940163\n",
      "    Current iteration=0, loss=11522.185582447979\n",
      "    Current iteration=1000, loss=407738.40106842207\n",
      "    Current iteration=0, loss=11522.185582447979\n",
      "    Current iteration=1000, loss=400043.5220470259\n",
      "    Current iteration=0, loss=11522.185582447979\n",
      "    Current iteration=1000, loss=52676.68261964835\n",
      "    Current iteration=0, loss=11522.185582447979\n",
      "    Current iteration=1000, loss=53938.23182821811\n",
      "    Current iteration=0, loss=11522.185582447979\n",
      "    Current iteration=1000, loss=54847.40348978257\n",
      "    Current iteration=0, loss=11522.185582447979\n",
      "    Current iteration=1000, loss=53398.92958882784\n",
      "    Current iteration=0, loss=11522.185582447979\n",
      "    Current iteration=1000, loss=199714.58428995305\n",
      "    Current iteration=0, loss=11522.185582447979\n",
      "    Current iteration=1000, loss=288817.8520658225\n",
      "    Current iteration=0, loss=11522.185582447979\n",
      "    Current iteration=1000, loss=204166.81367963974\n",
      "    Current iteration=0, loss=11522.185582447979\n",
      "    Current iteration=1000, loss=295001.6994260356\n",
      "    Current iteration=0, loss=11522.185582447979\n",
      "    Current iteration=1000, loss=1158434.3640883912\n",
      "    Current iteration=0, loss=11522.185582447979\n",
      "    Current iteration=1000, loss=1106712.889425164\n",
      "    Current iteration=0, loss=11522.185582447979\n",
      "    Current iteration=1000, loss=1150318.3211759643\n",
      "    Current iteration=0, loss=11522.185582447979\n",
      "    Current iteration=1000, loss=234771.58915037976\n",
      "    Current iteration=0, loss=11522.185582447979\n",
      "    Current iteration=1000, loss=249940.12311340982\n",
      "    Current iteration=0, loss=11522.185582447979\n",
      "    Current iteration=1000, loss=295177.60259270243\n",
      "    Current iteration=0, loss=11522.185582447979\n",
      "    Current iteration=1000, loss=121890.38244430503\n",
      "    Current iteration=0, loss=11522.185582447979\n",
      "    Current iteration=1000, loss=121997.80570233284\n",
      "    Current iteration=0, loss=11522.185582447979\n",
      "    Current iteration=1000, loss=228011.36902848337\n",
      "    Current iteration=0, loss=11522.185582447979\n",
      "    Current iteration=1000, loss=203658.27382846386\n",
      "    Current iteration=0, loss=11522.185582447979\n",
      "    Current iteration=1000, loss=241340.87257279744\n",
      "    Current iteration=0, loss=11522.185582447979\n",
      "    Current iteration=1000, loss=246659.21994235052\n",
      "    Current iteration=0, loss=11522.185582447979\n",
      "    Current iteration=1000, loss=410116.18022718607\n",
      "    Current iteration=0, loss=11522.185582447979\n",
      "    Current iteration=1000, loss=400000.44569936476\n",
      "    Current iteration=0, loss=11522.185582447979\n",
      "    Current iteration=1000, loss=461097.9309653159\n",
      "    Current iteration=0, loss=11522.185582447979\n",
      "    Current iteration=1000, loss=415470.40780295647\n",
      "    Current iteration=0, loss=11522.185582447979\n",
      "    Current iteration=1000, loss=367005.8811155245\n",
      "    Current iteration=0, loss=11522.185582447979\n",
      "    Current iteration=1000, loss=170689.2129025949\n",
      "    Current iteration=0, loss=11522.185582447979\n",
      "    Current iteration=1000, loss=194983.42501587048\n",
      "    Current iteration=0, loss=11522.185582447979\n",
      "    Current iteration=1000, loss=156241.08653186943\n",
      "    Current iteration=0, loss=11522.185582447979\n",
      "    Current iteration=1000, loss=200439.64365983888\n",
      "    Current iteration=0, loss=11522.185582447979\n",
      "    Current iteration=1000, loss=182272.9953915595\n",
      "    Current iteration=0, loss=11522.185582447979\n",
      "    Current iteration=1000, loss=206778.7446985168\n",
      "    Current iteration=0, loss=11522.185582447979\n",
      "    Current iteration=1000, loss=167003.3156298656\n",
      "    Current iteration=0, loss=11522.185582447979\n",
      "    Current iteration=1000, loss=279756.62932138366\n",
      "    Current iteration=0, loss=11522.185582447979\n",
      "    Current iteration=1000, loss=249107.68130380585\n",
      "    Current iteration=0, loss=11522.185582447979\n",
      "    Current iteration=1000, loss=225709.87226261004\n",
      "    Current iteration=0, loss=11522.185582447979\n",
      "    Current iteration=1000, loss=232587.76881242188\n",
      "    Current iteration=0, loss=11522.185582447979\n",
      "    Current iteration=1000, loss=333371.4804433245\n",
      "    Current iteration=0, loss=11522.185582447979\n",
      "    Current iteration=1000, loss=236793.92972551228\n",
      "    Current iteration=0, loss=11522.185582447979\n",
      "    Current iteration=1000, loss=229418.7658440142\n",
      "    Current iteration=0, loss=11522.185582447979\n",
      "    Current iteration=1000, loss=729030.2080496082\n",
      "    Current iteration=0, loss=11522.185582447979\n",
      "    Current iteration=1000, loss=179831.17824915843\n",
      "    Current iteration=0, loss=11522.185582447979\n",
      "    Current iteration=1000, loss=204179.63547536358\n",
      "    Current iteration=0, loss=11522.185582447979\n",
      "    Current iteration=1000, loss=187642.94150803512\n",
      "    Current iteration=0, loss=11522.185582447979\n",
      "    Current iteration=1000, loss=212053.5744781407\n",
      "    Current iteration=0, loss=11522.185582447979\n",
      "    Current iteration=1000, loss=197682.00176837528\n",
      "    Current iteration=0, loss=11522.185582447979\n",
      "    Current iteration=1000, loss=209387.8373616478\n",
      "    Current iteration=0, loss=11522.185582447979\n",
      "    Current iteration=1000, loss=221238.6241657489\n",
      "    Current iteration=0, loss=11522.185582447979\n",
      "    Current iteration=1000, loss=217823.50403884493\n",
      "    Current iteration=0, loss=11522.185582447979\n",
      "    Current iteration=1000, loss=365359.8451515826\n",
      "    Current iteration=0, loss=11522.185582447979\n",
      "    Current iteration=1000, loss=331456.71472989634\n",
      "    Current iteration=0, loss=11522.185582447979\n",
      "    Current iteration=1000, loss=304176.43304837006\n",
      "    Current iteration=0, loss=11522.185582447979\n",
      "    Current iteration=1000, loss=305606.9915255824\n",
      "##### [18021.36428514224, 75921.43156460201, 62616.57976327685, 57704.62387862448, 64466.13175198146] ####\n",
      "For JET_NB 3, the obtained best degree is 1 and gamma is 0.001 and loss is 18021.36428514224\n",
      "CPU times: user 2h 27min 16s, sys: 2min 1s, total: 2h 29min 18s\n",
      "Wall time: 55min 52s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "max_iters = 2_000\n",
    "seed = 15\n",
    "degrees = [1, 2, 3, 4, 5]\n",
    "k_fold = 4\n",
    "#gammas = [0.1, 0.01, 0.001, 0.0001]\n",
    "gammas = [0.1, 0.01, 0.001]\n",
    "\n",
    "\n",
    "\n",
    "for i in range(NUMBER_GROUPS):\n",
    "    y = Y_arr[i]\n",
    "    y[y == - 1.0] = 0.0\n",
    "    tx = TX_arr[i]\n",
    "    degree, gamma, rmse_min = cross_validation_demo_logistic(y, tx, max_iters, seed, degrees, k_fold, gammas)\n",
    "    print(f\"For JET_NB {i}, the obtained best degree is {degree} and gamma is {gamma} and loss is {rmse_min}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### REGULARIZED LOGISTIC REGRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "params = []\n",
    "\n",
    "for idx in range(4):\n",
    "    y=np.array(Y_arr[idx])\n",
    "    tX=np.array(TX_arr[idx])\n",
    "    initial_w = np.zeros(len(features_to_keep[idx]))\n",
    "    seed=15\n",
    "    degrees=[2, 4, 7]\n",
    "    k_fold=4\n",
    "    max_iters=200\n",
    "    lambdas = np.logspace(-4, 0, 10)\n",
    "    gammas = [0.01,0.001]\n",
    "    \n",
    "    tuple_ = cross_validation_demo_reg_logistic(y, tX, max_iters, seed, degrees, k_fold, lambdas, gammas)\n",
    "    params.append(tuple_)\n",
    "    print(\"group \",idx, \" tuple : \", tuple_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = '../data/test.csv' # TODO: download test data and supply path here \n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "jet_groups_indices_test = [tX_test[:, PRI_JET_NUM_IDX] == pri_jet_num_value for pri_jet_num_value in PRI_JET_NUM_VALUES]\n",
    "TX_test_arr = list([tX_test[group_indices] for group_indices in jet_groups_indices_test])\n",
    "\n",
    "#remove not used features\n",
    "TX_test_arr = [TX_test_arr[group_idx][:, features_to_keep[group_idx]] for group_idx in range(NUMBER_GROUPS)]\n",
    "\n",
    "#standardizing the data\n",
    "for tx in TX_test_arr:\n",
    "    tx -= np.mean(tx, axis = 0)\n",
    "    tx /= np.std(tx, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1. -1.  1. ...  1.  1. -1.]\n",
      "CPU times: user 8min 3s, sys: 28.7 s, total: 8min 32s\n",
      "Wall time: 2min 8s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "W_arr = []\n",
    "y_pred = np.empty(len(tX_test))\n",
    "\n",
    "for idx in range(4):\n",
    "    #weight, loss = least_squares_GD(Y_arr[idx], TX_arr[idx], np.zeros(TX_arr[idx].shape[1]), 10000, 0.001)\n",
    "    #weight, loss = least_squares(Y_arr[idx], TX_arr[idx])\n",
    "    #weight, loss = ridge_regression(Y_arr[idx], TX_arr[idx], 0.01)\n",
    "    y_pred[jet_groups_indices_test[idx]] = predict_labels(weight, TX_test_arr[idx])\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: fill in desired name of output file for submission\n",
    "OUTPUT_PATH = '../data/sample-submission.csv'\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = np.array([1, 2, 3])\n",
    "l = np.array([True, False, True, False, False, True])\n",
    "A = np.zeros(6)\n",
    "A[l] = Q\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = '../data/test.csv' # TODO: download test data and supply path here \n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PREPARE TEST DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jet_groups_indices = [tX_test[:, PRI_JET_NUM_IDX] == pri_jet_num_value for pri_jet_num_value in PRI_JET_NUM_VALUES]\n",
    "TX_test_arr = list([tX_test[group_indices] for group_indices in jet_groups_indices])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: fill in desired name of output file for submission\n",
    "OUTPUT_PATH = '../data/sample-submission.csv'\n",
    "y_pred = predict_labels(weights, tX_test)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
